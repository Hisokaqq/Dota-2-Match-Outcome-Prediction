# -*- coding: utf-8 -*-
"""Dota2_presiction_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fLFOMoLz7Ca9JXYA7uWO7Kiz1Zv1RY4v

# Dota 2 Match Outcome Prediction  

This project explores the relationship between hero selection and match outcomes in Dota 2, focusing on the unique abilities, synergies, and counters of each hero. By analyzing historical match data and leveraging machine learning, the goal is to predict match outcomes and provide insights into how hero picks impact a team's chances of success.

---

### Goals  

1. **Analyze Hero Interactions**: Examine how heroes interact through counters and synergies, as well as the influence of win rates on match outcomes.  
2. **Predict Match Results**: Develop a machine learning model to predict match winners based on hero selections alone.  
3. **Identify Key Features**: Determine which factors—such as win rate differences, counter-matchups, or team synergy—play the biggest role in shaping outcomes.  

---

### Key Points  

- The analysis focuses on **hero picks and their interactions**, emphasizing the role of counters and team synergy.  
- Predictions are based solely on hero data and do not account for external factors like player skill, strategy, or in-game dynamics.  
- The project aims to uncover how much information hero data alone provides about match outcomes.  

---

### Approach  

1. **Data Collection**  
   - Match data was analyzed to calculate hero-specific metrics, such as win rates, counters, and synergies. These metrics were derived from historical matches, ensuring they were customized and up-to-date for this project.  
   - Data was gathered using the [OpenDota API](https://docs.opendota.com/) to source raw match information.  

2. **Data Processing**  
   - Processed match data with techniques like scaling and imputation to clean and standardize inputs.  
   - Engineered features like win rate differences, matchup scores, and synergy scores to capture the complexity of hero interactions.  

3. **Modeling**  
   - Developed predictive models, including Random Forest, Gradient Boosting and Logistic Regression, to forecast match outcomes.  
   - The models incorporated **hero counters** and **team synergies** as key inputs, alongside win rates, to improve accuracy.  

4. **Evaluation and Optimization**  
   - Evaluated model performance using metrics like accuracy and confidence scores.  
   - Fine-tuned model parameters through hyperparameter optimization to enhance predictions.  

---

### Results and Insights  

The project highlights the significant role of hero selection in Dota 2 matches, offering insights into how counters and synergies influence outcomes. While predictions cannot account for external factors like player skill, the models provide a solid foundation for understanding the strategic importance of hero picks.
"""

!pip install pandas numpy matplotlib plotly seaborn scikit-learn --quiet

# Commented out IPython magic to ensure Python compatibility.
import os
import matplotlib
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
# %matplotlib inline
sns.set_style('darkgrid')
matplotlib.rcParams['font.size'] = 14
matplotlib.rcParams['figure.figsize'] = (10, 6)
matplotlib.rcParams['figure.facecolor'] = '#00000000'
pd.set_option('display.max_columns', None)

"""To skip API fetching, scroll down to the section titled **"Load Preprocessed Data"**.

## Data Source

The project utilizes the **OpenDota API** for gathering data to analyze hero interactions and match outcomes. Two key endpoints are used:

```python
# Fetch hero data (ID and names)
response = requests.get("https://api.opendota.com/api/heroStats")

# Fetch match data
response = requests.get("https://api.opendota.com/api/publicMatches")
```

### Overview:
1. **Hero Data**:
   - Collect hero IDs and localized names.
   - Use this data to build matrices for synergies (how well heroes perform together) and counters (how effective they are against specific heroes).

2. **Match Data**:
   - Fetch public matches to populate match structures, e.g.:
     ```python
     "radiant_team": heroes in the Radiant team,
     "dire_team": heroes in the Dire team,
     "radiant_win": match result.
     ```
   - Many fields (e.g., synergy and matchup scores) are initialized as `0` since the API doesn't provide them directly.
   - These values are calculated by analyzing hero interactions across multiple matches to fill the hero matrices.

This approach ensures comprehensive data processing for hero matchups and synergies, forming the basis for accurate predictions.

### Data Source: Hero Information

Hero data is fetched from the OpenDota API (`/heroStats`) to get IDs and names. This data is used to initialize matrices for hero matchups and synergies, which are populated later with match data.
"""

import requests


url = "https://api.opendota.com/api/heroStats"
response = requests.get(url)

heroes = []

if response.status_code == 200:
    heroes2 = response.json()
    for hero in heroes2:
        heroes.append({"id": hero["id"], "name": hero["localized_name"], "num_games": 0, "win_games": 0})

heroes_df = pd.DataFrame(heroes)

num_heroes = len(heroes)

hero_matchups_df = pd.DataFrame(
    data=np.zeros((num_heroes, num_heroes)),
    index=[hero["id"] for hero in heroes],
    columns=[hero["id"] for hero in heroes]
)
hero_matchups_count_df = hero_matchups_df.copy()
hero_matchups = hero_matchups_df.to_dict(orient='records')
hero_matchups_count = hero_matchups_count_df.to_dict(orient='records')

hero_synergies_df = hero_matchups_df.copy()
hero_synergies_count_df = hero_synergies_df.copy()
hero_synergies = hero_synergies_df.to_dict(orient='records')
hero_synergies_count = hero_synergies_count_df.to_dict(orient='records')

"""### Data Source: Match Information

The script fetches match data using the OpenDota API (`/publicMatches`). The primary goal is to collect basic match attributes and store unpopulated matches for further processing. The collected attributes include:

- **Populated Attributes**:
  - `match_id`: Unique identifier for each match.
  - `radiant_team`: List of heroes on the Radiant team (if available).
  - `dire_team`: List of heroes on the Dire team (if available).
  - `radiant_win`: Indicates whether Radiant won the match.

- **Unpopulated (Defaulted to 0)**:
  - `radiant_wr`: Radiant team's win rate.
  - `dire_wr`: Dire team's win rate.
  - `radiant_matchups_score`: Radiant's matchup score based on hero counters.
  - `dire_matchups_score`: Dire's matchup score based on hero counters.
  - `radiant_synergies_score`: Radiant's synergy score based on team composition.
  - `dire_synergies_score`: Dire's synergy score based on team composition.

This step focuses on gathering raw match data, which is later enriched by calculating scores and rates based on hero performance.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# import requests
# import pandas as pd
# from time import sleep
# 
# base_url = "https://api.opendota.com/api/publicMatches"
# 
# columns = ["match_id", "radiant_team", "dire_team", "radiant_win"]
# matches_df = pd.DataFrame(columns=columns)
# 
# target_matches = 200000
# fetched_matches = 0
# last_match_id = None
# 
# max_retries = 5
# 
# while fetched_matches < target_matches:
#     params = {"less_than_match_id": last_match_id} if last_match_id else {}
# 
#     response = requests.get(base_url, params=params)
# 
#     if response.status_code == 200:
#         matches = response.json()
# 
#         if not matches:
#             print("No more matches to fetch.")
#             break
# 
#         for match in matches:
#             match_data = {
#                 "match_id": match["match_id"],
#                 "radiant_team": match.get("radiant_team", None),
#                 "dire_team": match.get("dire_team", None),
#                 "radiant_wr": 0,
#                 "dire_wr": 0,
#                 "radiant_matchups_score": 0,
#                 "dire_matchups_score": 0,
#                 "radiant_synergies_score": 0,
#                 "dire_synergies_score": 0,
#                 "radiant_win": match["radiant_win"],
#             }
#             matches_df = pd.concat([matches_df, pd.DataFrame([match_data])], ignore_index=True)
# 
#         last_match_id = matches[-1]["match_id"]
#         fetched_matches += len(matches)
# 
#         print(f"Fetched {fetched_matches} matches...")
#         sleep(1)
# 
#     elif response.status_code == 429:
#         print(f"Rate limit exceeded. Retrying in 20 seconds.../or exiting")
#         break #it will go on continuesly if use continue
# 
#     else:
#         print(f"Error {response.status_code}: {response.text}")
#         max_retries -= 1
#         if max_retries == 0:
#             print("Maximum retries reached. Exiting.")
#             break
#         sleep(10)
#         continue

"""### Populating Hero Matchups, Synergies, and Win Rates

In this part of the process, we populate the following data:

1. **Hero Win Rates**:
   - Track the number of games played and won for each hero.
   - Update win counts based on match outcomes (Radiant or Dire victory).

2. **Hero Matchups**:
   - Update how heroes perform against each other.
   - Populate a matrix reflecting the net score of hero interactions and the total count of matchups.

3. **Hero Synergies**:
   - Analyze how heroes perform when on the same team.
   - Update a synergy matrix to reflect win/loss contributions for team combinations and the total synergy count.

This step processes each match iteratively and updates these metrics to create a comprehensive dataset for further analysis.
"""

def update_hero_matchups(hero_matchups_df, hero_matchups_count_df, radiant_team, dire_team, radiant_win):

    for radiant_hero in radiant_team:
        for dire_hero in dire_team:
            if radiant_win:
                hero_matchups_df.loc[radiant_hero, dire_hero] += 1
                hero_matchups_df.loc[dire_hero, radiant_hero] -= 1
            else:
                hero_matchups_df.loc[radiant_hero, dire_hero] -= 1
                hero_matchups_df.loc[dire_hero, radiant_hero] += 1

            hero_matchups_count_df.loc[radiant_hero, dire_hero] += 1
            hero_matchups_count_df.loc[dire_hero, radiant_hero] += 1

def update_hero_synergies(hero_synergies_df, hero_synergies_count_df, team, win):

    for hero1 in team:
        for hero2 in team:
            if hero1 != hero2:
                if win:
                    hero_synergies_df.loc[hero1, hero2] += 1
                else:
                    hero_synergies_df.loc[hero1, hero2] -= 1

                hero_synergies_count_df.loc[hero1, hero2] += 1

# Commented out IPython magic to ensure Python compatibility.
# %%time
# for _, row in matches_df.iterrows():
#     radiant_team = row["radiant_team"]
#     dire_team = row["dire_team"]
#     radiant_win = row["radiant_win"]
# 
#     for hero in radiant_team:
#         heroes_df.loc[heroes_df["id"] == hero, "num_games"] += 1
#         heroes_df.loc[heroes_df["id"] == hero, "win_games"] += 1 if radiant_win else 0
# 
#     for hero in dire_team:
#         heroes_df.loc[heroes_df["id"] == hero, "num_games"] += 1
#         heroes_df.loc[heroes_df["id"] == hero, "win_games"] += 0 if radiant_win else 1
# 
#     update_hero_matchups(hero_matchups_df, hero_matchups_count_df, radiant_team, dire_team, radiant_win)
# 
#     update_hero_synergies(hero_synergies_df, hero_synergies_count_df, radiant_team, radiant_win)
#     update_hero_synergies(hero_synergies_df, hero_synergies_count_df, dire_team, not radiant_win)
# 
#     match_data = {
#         "match_id": row["match_id"],
#         "radiant_team": radiant_team,
#         "dire_team": dire_team,
# 
#         "radiant_win": radiant_win,
#     }
# 
# 
#     # print(match_data)

hero_synergies_df.head()

hero_synergies_count_df.head()

heroes_df.head()
heroes_df["winrate"] = heroes_df["win_games"] / heroes_df["num_games"] * 100
heroes_df.sort_values(by="winrate", ascending=True).head()
heroes_df

"""### Populating Unpopulated Match Data

With the previously populated hero statistics, matchups, and synergies, this step fills in the unpopulated attributes for matches. Specifically, the following data is calculated and updated for each match:

1. **Team Win Rates**:
   - **Radiant Win Rate (`radiant_wr`)**: The average win rate of all heroes on the Radiant team.
   - **Dire Win Rate (`dire_wr`)**: The average win rate of all heroes on the Dire team.

2. **Matchup Scores**:
   - **Radiant Matchup Score (`radiant_matchups_score`)**: Measures how well Radiant heroes perform against Dire heroes based on historical matchups.
   - **Dire Matchup Score (`dire_matchups_score`)**: Measures how well Dire heroes perform against Radiant heroes.

3. **Synergy Scores**:
   - **Radiant Synergy Score (`radiant_synergies_score`)**: The average synergy score of heroes within the Radiant team.
   - **Dire Synergy Score (`dire_synergies_score`)**: The average synergy score of heroes within the Dire team.

This ensures that each match entry is fully enriched with calculated statistics, making it ready for analysis or model input.
"""

def process_match_data(matches_df, heroes_df, hero_matchups_df, hero_matchups_count_df, hero_synergies_df, hero_synergies_count_df):

    for idx, row in matches_df.iterrows():
        radiant_team = row["radiant_team"]
        dire_team = row["dire_team"]

        if not radiant_team or not dire_team:
            matches_df.at[idx, "radiant_wr"] = 0
            matches_df.at[idx, "dire_wr"] = 0
            matches_df.at[idx, "radiant_matchups_score"] = 0
            matches_df.at[idx, "dire_matchups_score"] = 0
            matches_df.at[idx, "radiant_synergies_score"] = 0
            matches_df.at[idx, "dire_synergies_score"] = 0
            continue

        radiant_stats = heroes_df[heroes_df["id"].isin(radiant_team)]
        dire_stats = heroes_df[heroes_df["id"].isin(dire_team)]

        radiant_wr = (
            radiant_stats.apply(lambda x: x["win_games"] / x["num_games"] if x["num_games"] > 0 else 0, axis=1).mean()
            * 100
        )

        dire_wr = (
            dire_stats.apply(lambda x: x["win_games"] / x["num_games"] if x["num_games"] > 0 else 0, axis=1).mean()
            * 100
        )

        radiant_matchups_score = 0
        dire_matchups_score = 0

        for radiant_hero in radiant_team:
            for dire_hero in dire_team:
                radiant_count = hero_matchups_count_df.loc[radiant_hero, dire_hero]
                if radiant_count > 0:
                    radiant_score = hero_matchups_df.loc[radiant_hero, dire_hero]
                    radiant_matchups_score += radiant_score / radiant_count

                dire_count = hero_matchups_count_df.loc[dire_hero, radiant_hero]
                if dire_count > 0:
                    dire_score = hero_matchups_df.loc[dire_hero, radiant_hero]
                    dire_matchups_score += dire_score / dire_count

        radiant_synergies_score = 0
        dire_synergies_score = 0

        for i, radiant_hero1 in enumerate(radiant_team):
            for radiant_hero2 in radiant_team[i + 1:]:
                radiant_synergy_count = hero_synergies_count_df.loc[radiant_hero1, radiant_hero2]
                if radiant_synergy_count > 0:
                    radiant_synergy_score = hero_synergies_df.loc[radiant_hero1, radiant_hero2]
                    radiant_synergies_score += radiant_synergy_score / radiant_synergy_count

        for i, dire_hero1 in enumerate(dire_team):
            for dire_hero2 in dire_team[i + 1:]:
                dire_synergy_count = hero_synergies_count_df.loc[dire_hero1, dire_hero2]
                if dire_synergy_count > 0:
                    dire_synergy_score = hero_synergies_df.loc[dire_hero1, dire_hero2]
                    dire_synergies_score += dire_synergy_score / dire_synergy_count

        matches_df.at[idx, "radiant_wr"] = radiant_wr
        matches_df.at[idx, "dire_wr"] = dire_wr
        matches_df.at[idx, "radiant_matchups_score"] = radiant_matchups_score
        matches_df.at[idx, "dire_matchups_score"] = dire_matchups_score
        matches_df.at[idx, "radiant_synergies_score"] = radiant_synergies_score
        matches_df.at[idx, "dire_synergies_score"] = dire_synergies_score

# Commented out IPython magic to ensure Python compatibility.
# %%time
# process_match_data(matches_df, heroes_df, hero_matchups_df, hero_matchups_count_df, hero_synergies_df, hero_synergies_count_df)
# 
# matches_df.to_csv("updated_matches.csv", index=False)
# matches_df

"""### Data Saving and Loading

This section ensures that all processed data is securely stored and can be easily reloaded for further analysis or model training.

1. **Saving Data**:
   - All DataFrames, including heroes, matchups, synergies, and matches, are saved as CSV files in a `data` folder. This preserves the computed statistics and ensures reusability.

2. **Loading Data**:
   - The saved CSV files are reloaded into DataFrames to verify their integrity and to make them readily available for subsequent steps.

This approach provides a streamlined workflow for managing large datasets, avoiding redundant computations, and ensuring consistency across sessions.
"""

import os

os.makedirs("data", exist_ok=True)

heroes_df.to_csv("data/heroes.csv", index=False)
hero_matchups_df.to_csv("data/hero_matchups.csv", index=True)
hero_matchups_count_df.to_csv("data/hero_matchups_count.csv", index=True)
hero_synergies_df.to_csv("data/hero_synergies.csv", index=True)
hero_synergies_count_df.to_csv("data/hero_synergies_count.csv", index=True)
matches_df.to_csv("data/matches.csv", index=False)

print("All DataFrames have been saved to the 'data' folder.")

"""#### Load Preprocessed Data"""

heroes_df = pd.read_csv("data/heroes.csv")
hero_matchups_df = pd.read_csv("data/hero_matchups.csv", index_col=0)
hero_matchups_count_df = pd.read_csv("data/hero_matchups_count.csv", index_col=0)
hero_synergies_df = pd.read_csv("data/hero_synergies.csv", index_col=0)
hero_synergies_count_df = pd.read_csv("data/hero_synergies_count.csv", index_col=0)
matches_df = pd.read_csv("data/matches.csv")

print("All DataFrames have been loaded from the 'data' folder.")

"""### Index and Column Adjustment

Ensures all indices and columns in matchup and synergy DataFrames are of type `int` for consistency and compatibility during computations.
"""

hero_matchups_df.index = hero_matchups_df.index.astype(int)
hero_matchups_df.columns = hero_matchups_df.columns.astype(int)
hero_matchups_count_df.index = hero_matchups_count_df.index.astype(int)
hero_matchups_count_df.columns = hero_matchups_count_df.columns.astype(int)
hero_synergies_df.index = hero_synergies_df.index.astype(int)
hero_synergies_df.columns = hero_synergies_df.columns.astype(int)
hero_synergies_count_df.index = hero_synergies_count_df.index.astype(int)
hero_synergies_count_df.columns = hero_synergies_count_df.columns.astype(int)

"""### Test Match Creation and Processing

Creates a test match with valid hero IDs, initializes its attributes, and processes it using existing hero data and matchup matrices.
"""

valid_radiant_team = [hero for hero in [1, 2] if hero in hero_matchups_df.index]
valid_dire_team = [hero for hero in [3, 4] if hero in hero_matchups_df.index]

if len(valid_radiant_team) < 2 or len(valid_dire_team) < 2:
    raise ValueError("One or more hero IDs are missing in the matchup DataFrames. Ensure valid IDs are used.")

test_match = {
    "match_id": 1,
    "radiant_team": valid_radiant_team,
    "dire_team": valid_dire_team,
    "radiant_wr": 0,
    "dire_wr": 0,
    "radiant_matchups_score": 0,
    "dire_matchups_score": 0,
    "radiant_synergies_score": 0,
    "dire_synergies_score": 0,
    "radiant_win": True
}

test_matches_df = pd.DataFrame([test_match])

process_match_data(
    matches_df=test_matches_df,
    heroes_df=heroes_df,
    hero_matchups_df=hero_matchups_df,
    hero_matchups_count_df=hero_matchups_count_df,
    hero_synergies_df=hero_synergies_df,
    hero_synergies_count_df=hero_synergies_count_df
)

test_matches_df

# heroes_df.head()
# hero_synergies_df.head()
# hero_matchups_df.head()

# hero_synergies_count_df.head()
# hero_matchups_count_df.head()

"""## Analyse of Data

### Analyse of Hero Data
"""

hero_id_counts = heroes_df["id"].value_counts()
duplicate_hero_ids = hero_id_counts[hero_id_counts > 1]

print("Duplicate Game IDs:")
print(duplicate_hero_ids)
print("Number of Heroes: ", len(heroes_df))

heroes_df.head()

heroes_df.info()

heroes_df.describe()

"""#### Heatmap Visualization

Generates heatmaps to visualize hero matchup and synergy correlations based on the processed data. These highlight the effectiveness of heroes against opponents and their synergy with teammates.
"""

plt.figure(figsize=(10, 10))
matchups_corr = (hero_matchups_df/hero_matchups_count_df).fillna(0)
sns.heatmap(matchups_corr, cmap="coolwarm")
plt.title("Hero Matchup Correlations")
plt.show()

plt.figure(figsize=(10, 10))
synergies_corr = (hero_synergies_df/hero_synergies_count_df).fillna(0)
sns.heatmap(synergies_corr, cmap="coolwarm")
plt.title("Hero Synergies Correlations")
plt.show()

"""#### Winrate distribution
The histogram visualizes the **normal distribution of heroes' winrates**, which typically range from **42% to 56%**. This range can vary depending on the patch, as balance changes can make some heroes temporarily too strong or too weak.

- **Outliers**: Heroes with winrates near the extremes (e.g., very high or very low) may reflect imbalance. This could also result in skewness, where one of the tails of the distribution rises instead of falling, a phenomenon known as **positive or negative skew**.
  
- **Smaller Winrate Range**: A smaller range of winrates is desirable, as it indicates better balance. It means each hero has a fair chance to win.

- **Strong Heroes with Low Winrates**: Sometimes, strong heroes may have lower winrates, not due to weakness but because they are **difficult to play effectively**, leading to more misuse by less experienced players.

This visualization provides insights into the current balance state of the game and how heroes perform overall.
"""

sns.histplot(data=heroes_df, x="winrate", bins=30, kde=True)

"""### Analyse of Games Data"""

game_id_counts = matches_df["match_id"].value_counts()
duplicate_game_ids = game_id_counts[game_id_counts > 1]

print("Duplicate Game IDs:")
print(duplicate_game_ids)
print("Number of Heroes: ", len(matches_df))

matches_df.head()

matches_df.info()

matches_df.describe()

"""#### Radiant and Dire wins distribution"""

sns.countplot(data=matches_df, x='radiant_win', palette='viridis')

plt.title('Count Radient Wins')
plt.xlabel('Radient Won?')
plt.ylabel('Count')
plt.show()

comparison_df = matches_df[['radiant_matchups_score', 'dire_matchups_score', 'radiant_wr', 'dire_wr', 'radiant_win']].copy()
comparison_df['wr_diff'] = comparison_df['radiant_wr'] - comparison_df['dire_wr']
print("Winrate differences:")
print(comparison_df['wr_diff'].describe())

"""#### Win Rate Comparison

Calculates and compares the number of matches where the win rate difference (`wr_diff`) aligns positively or negatively with the match outcome, grouping by Radiant and Dire wins.
"""

df_team1 = comparison_df[comparison_df["radiant_win"] == True]

pos_wr = len(df_team1[df_team1['wr_diff'] > 0])
neg_wr = len(df_team1[df_team1['wr_diff'] <= 0])

df_team2 = comparison_df[comparison_df["radiant_win"] == False]

pos_wr += len(df_team2[df_team2['wr_diff'] < 0])
neg_wr += len(df_team2[df_team2['wr_diff'] >= 0])

comparison = {
    "Positive WR": pos_wr,
    "Negative WR": neg_wr
}

comparison

"""## Preparing Data for Prediction Models

### Defining Input and Target Columns

In this block, we specify the **input features** (`input_cols`) and the **target variable** (`target_col`) for the machine learning model:

- **Input Features (`input_cols`)**: These are the metrics used to predict the outcome of the game:
  - `radiant_wr`: The average win rate of heroes on the Radiant team.
  - `dire_wr`: The average win rate of heroes on the Dire team.
  - `radiant_matchups_score`: The score reflecting how well the Radiant heroes counter the Dire heroes.
  - `dire_matchups_score`: The score reflecting how well the Dire heroes counter the Radiant heroes.
  - `radiant_synergies_score`: The synergy score within the Radiant team.
  - `dire_synergies_score`: The synergy score within the Dire team.

- **Target Variable (`target_col`)**: This is the outcome we aim to predict (`radiant_win`), indicating whether the **Radiant team** or the **Dire team** won the game.

This setup ensures the model learns from hero win rates, counter matchups, and team synergies to make accurate predictions.
"""

matches_df.head()

input_cols = ['radiant_wr', 'dire_wr', 'radiant_matchups_score', 'dire_matchups_score', 'radiant_synergies_score', 'dire_synergies_score']
target_col = 'radiant_win'

"""#### Splitting the Data into Training and Testing Sets

This step divides the dataset into **training** (80%) and **testing** (20%) subsets using `train_test_split`. This ensures that the model is trained on one portion of the data and tested on a separate portion to evaluate its performance.
"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(matches_df[input_cols], matches_df[target_col], test_size=0.2, random_state=42)
y_train = y_train.astype(bool)
y_test = y_test.astype(bool)

"""#### Data Preprocessing: Imputation and Scaling

This block preprocesses the data in two steps to prepare it for model training:

1. **Imputation with SimpleImputer**:
   - Handles missing values by replacing them with the **mean** of each column in the training set.
   - Even though there are no missing values in this dataset (since it was created manually), this step is included as a precaution.

2. **Feature Scaling with MinMaxScaler**:
   - Scales features to a range of **0 to 1** to normalize the data.
   - Prevents features with large ranges from dominating the model's performance.

These steps standardize the data and ensure it's robust and ready for machine learning algorithms.
"""

from sklearn.impute import SimpleImputer
imputer = SimpleImputer(strategy="mean")
imputer.fit(X_train)
X_train = imputer.transform(X_train)
X_test = imputer.transform(X_test)

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
scaler.fit(X_train)
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)

"""### Comparing Simple Prediction Strategies

This section explores three different strategies for predicting the winning team and evaluates their performance:

1. **Random Prediction (`compare_flip_coin`)**:
   - Simulates a coin flip to randomly assign a winner between "team1" and "team2".
   - Incorporates a random seed for reproducibility.

2. **Win Rate-Based Prediction (`compare_wr`)**:
   - Compares the win rates of both teams.
   - Predicts "team1" if its win rate is equal to or greater than "team2"; otherwise, predicts "team2".
"""

from sklearn.metrics import accuracy_score, classification_report

import random
def compare_flip_coin(inputs, seed=None):

    if seed is not None:
        random.seed(seed)

    predictions = []

    for i in inputs:
        if random.randint(0, 1) == 1:

            predictions.append(True)
            continue
        predictions.append(False)
    return predictions

predictions = pd.DataFrame({"radiant_win": compare_flip_coin(X_train, seed=42)})
predictions_series = predictions["radiant_win"].astype(bool)
accuracy = accuracy_score(y_train, predictions_series)
print(f"Accuracy: {accuracy:.2f}")

def compare_wr(inputs):
  pd_inputs = pd.DataFrame(inputs, columns=input_cols)
  predictions = []

  for _, row in pd_inputs.iterrows():
    if row["radiant_wr"] >= row["dire_wr"]:
      predictions.append(True)
      continue
    predictions.append(False)
  return predictions

predictions = pd.DataFrame({"radiant_win": compare_wr(X_train)})
predictions_series = predictions["radiant_win"].astype(bool)
accuracy = accuracy_score(y_train, predictions)
print(f"Accuracy: {accuracy:.2f}")

"""### Random Forest Classifier

The **Random Forest Classifier (RFC)** works by creating multiple decision trees on random subsets of data and features, then combining their outputs to make a final prediction, reducing overfitting and improving accuracy.

#### Why it Fits Our Data
RFC is ideal for our dataset because:
- It handles non-linear relationships between hero matchups, win rates, and outcomes.
- It’s robust to noisy data, which is crucial given the variability in hero interactions.
- It provides feature importance, helping us understand which factors (e.g., win rate or score) impact predictions the most.

This makes it a strong choice for predicting match outcomes based on hero picks.

[More about RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)
"""

from sklearn.ensemble import RandomForestClassifier
rfc = RandomForestClassifier(n_estimators=20, random_state=42)
rfc.fit(X_train, y_train)

train_preds = rfc.predict(X_test)
accuracy = accuracy_score(y_test, train_preds)
print(f"Accuracy: {accuracy:.2f}")

"""#### Evaluating the Impact of `n_estimators`

This block evaluates how the number of decision trees (`n_estimators`) in a Random Forest Classifier affects its performance. For each value of `n_estimators` from 1 to 99:
- The model is trained and tested.
- **Test Accuracy**: Measures the model's performance on unseen data.
- **Train Accuracy**: Measures performance on the training set, indicating overfitting if too high.

The results are visualized in a plot showing how increasing the number of trees impacts the accuracies. This helps identify an optimal value for `n_estimators` where the model balances generalization and accuracy.
"""

def best_n_estimator(n_estimator):
  rfc = RandomForestClassifier(n_estimators=n_estimator, random_state=42)
  rfc.fit(X_train, y_train)
  train_preds = rfc.predict(X_test)
  test_preds = rfc.predict(X_train)
  return {"N Estimators": n_estimator, "Test Accuracy": accuracy_score(y_test, train_preds), "Train Accuracy": accuracy_score(y_train, test_preds)}

# Commented out IPython magic to ensure Python compatibility.
# %%time
# accuracie_df = pd.DataFrame([best_n_estimator(i) for i in range(1, 100)])

plt.plot(accuracie_df["N Estimators"], accuracie_df["Test Accuracy"], label="Test Accuracy")
plt.plot(accuracie_df["N Estimators"], accuracie_df["Train Accuracy"], label="Train Accuracy")
plt.xlabel("N Estimators")
plt.ylabel("Accuracy")
plt.title("Accuracy vs N Estimators")
plt.legend()
plt.show()

accuracie_df.sort_values(by="Test Accuracy", ascending=False).head()

"""#### Analyzing Optimal Hyperparameters

This block focuses on finding the best configuration of hyperparameters for the Random Forest model, specifically the number of estimators (`n_estimators`).

- The function `best_n_estimator` trains the model with a specified number of decision trees, evaluating its performance on both training and test datasets.
- A range of values (1 to 99) for `n_estimators` is tested.
- **Train Accuracy** and **Test Accuracy** are calculated for each configuration and stored in a DataFrame.

The generated plot compares training and test accuracy as the number of estimators increases, revealing trends in model performance and identifying the point where adding more trees no longer improves generalization. This approach ensures the model is efficient without overfitting.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# def evaluate_model(n_estimators, max_depth, max_features):
#     rfc = RandomForestClassifier(
#         n_estimators=n_estimators,
#         max_depth=max_depth,
#         max_features=max_features,
#         random_state=42
#     )
#     rfc.fit(X_train, y_train)
#     test_preds = rfc.predict(X_test)
#     train_preds = rfc.predict(X_train)
#     return {
#         "N Estimators": n_estimators,
#         "Max Depth": max_depth,
#         "Max Features": max_features,
#         "Test Accuracy": accuracy_score(y_test, test_preds),
#         "Train Accuracy": accuracy_score(y_train, train_preds)
#     }
# 
# param_grid = [
#     (n, d, f)
#     for n in range(10, 101, 10)
#     for d in range(1, 21, 5)
#     for f in range(1, X_train.shape[1] + 1, 2)
# ]
# 
# results = [evaluate_model(n, d, f) for n, d, f in param_grid]
# 
# results_df = pd.DataFrame(results)
# 
# fig = px.scatter_3d(
#     results_df,
#     x="N Estimators",
#     y="Max Depth",
#     z="Max Features",
#     color="Test Accuracy",
#     size="Test Accuracy",
#     title="Hyperparameter Tuning Results",
#     labels={"N Estimators": "N Estimators", "Max Depth": "Max Depth", "Max Features": "Max Features"}
# )
# fig.show()

best_rf_params = results_df.loc[results_df["Test Accuracy"].idxmax()]
print("Best Random Forest Hyperparameters: ", best_rf_params.to_dict())

n_estimators = int(best_rf_params.to_dict()["N Estimators"])
max_depth = int(best_rf_params.to_dict()["Max Depth"])
max_features = int(best_rf_params.to_dict()["Max Features"])

rfc = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, max_features=max_features, random_state=42)
rfc.fit(X_train, y_train)

importance_df = pd.DataFrame({
    "Feature": input_cols,
    "Importance": rfc.feature_importances_
}).sort_values(by="Importance", ascending=False)
importance_df

"""### Gradient Boosting Hyperparameter Tuning

This block explores the optimal hyperparameters for a **Gradient Boosting Classifier**, a powerful ensemble learning method that builds models sequentially to correct errors made by prior models.

- **Key Parameters Tuned**:
  - `n_estimators`: Number of boosting stages (range: 10–100).
  - `max_depth`: Maximum depth of each tree (range: 1–10).
  - `learning_rate`: Shrinks the contribution of each tree (values: 0.01, 0.1, 0.2).

- **Process**:
  - The `evaluate_gb_model` function calculates the accuracy of the model on both training and test sets for each combination of parameters.
  - Results are stored in a DataFrame and visualized in a 3D scatter plot, showing how different hyperparameters influence **Test Accuracy**.

- **Why Gradient Boosting?**:
  Gradient Boosting works well with tabular data and provides strong performance by iteratively correcting mistakes. It’s particularly suited for datasets where capturing subtle patterns is crucial, such as this one with hero matchups and scores.

The optimal configuration is determined by selecting the hyperparameters with the highest test accuracy, ensuring balance between accuracy and generalization.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# from sklearn.ensemble import GradientBoostingClassifier
# 
# def evaluate_gb_model(n_estimators, max_depth, learning_rate):
#     gbc = GradientBoostingClassifier(
#         n_estimators=n_estimators,
#         max_depth=max_depth,
#         learning_rate=learning_rate,
#         random_state=42
#     )
#     gbc.fit(X_train, y_train)
#     test_preds = gbc.predict(X_test)
#     train_preds = gbc.predict(X_train)
#     return {
#         "N Estimators": n_estimators,
#         "Max Depth": max_depth,
#         "Learning Rate": learning_rate,
#         "Test Accuracy": accuracy_score(y_test, test_preds),
#         "Train Accuracy": accuracy_score(y_train, train_preds)
#     }
# 
# param_grid_gb = [
#     (n, d, lr)
#     for n in range(10, 101, 10)
#     for d in range(1, 11, 2)
#     for lr in [0.01, 0.1, 0.2]
# ]
# 
# results_gb = [evaluate_gb_model(n, d, lr) for n, d, lr in param_grid_gb]
# 
# results_gb_df = pd.DataFrame(results_gb)
# 
# fig_gb = px.scatter_3d(
#     results_gb_df,
#     x="N Estimators",
#     y="Max Depth",
#     z="Learning Rate",
#     color="Test Accuracy",
#     size="Test Accuracy",
#     title="Gradient Boosting Hyperparameter Tuning",
#     labels={"N Estimators": "N Estimators", "Max Depth": "Max Depth", "Learning Rate": "Learning Rate"}
# )
# fig_gb.show()

best_gb_params = results_gb_df.loc[results_gb_df["Test Accuracy"].idxmax()]
print("Best Gradient Boosting Hyperparameters: ", best_gb_params.to_dict())

"""### Logistic Regression Hyperparameter Tuning

This block focuses on identifying the best hyperparameters for **Logistic Regression**, a simple yet effective model for binary classification. Logistic Regression is especially suitable for interpreting the relationships between input features and the target variable, making it ideal for this project’s focus on hero matchups and team synergies.

- **Key Parameters Tuned**:
  - `penalty`: The type of regularization applied (`l1`, `l2`, `elasticnet`, or `none`).
  - `C`: Inverse of regularization strength (values: 0.01, 0.1, 1, 10, 100).
  - `solver`: Optimization algorithm used (`liblinear`, `saga`, or `lbfgs`).

- **Process**:
  - The `evaluate_logistic_model` function computes the model’s performance on both training and test datasets for each parameter combination.
  - Results are stored in a DataFrame and visualized in a 3D scatter plot, providing insights into how different configurations affect **Test Accuracy**.

- **Why Logistic Regression?**:
  Logistic Regression provides a balance between interpretability and predictive power. Its straightforward structure allows for easy understanding of how hero performance metrics like win rates and matchup scores contribute to match outcomes.

The best hyperparameters are chosen based on the highest test accuracy, ensuring a model that generalizes well while maintaining interpretability.
"""

from sklearn.linear_model import LogisticRegression

def evaluate_logistic_model(C, solver, max_iter):
    model = LogisticRegression(C=C, solver=solver, max_iter=max_iter, random_state=42)
    model.fit(X_train, y_train)
    test_preds = model.predict(X_test)
    train_preds = model.predict(X_train)
    return {
        "C": C,
        "Solver": solver,
        "Max Iter": max_iter,
        "Test Accuracy": accuracy_score(y_test, test_preds),
        "Train Accuracy": accuracy_score(y_train, train_preds)
    }

C_values = [0.01, 0.1, 1, 10, 100]
solvers = ['lbfgs', 'liblinear', 'saga']
max_iters = [100, 200, 300]

param_grid = [
    (C, solver, max_iter)
    for C in C_values
    for solver in solvers
    for max_iter in max_iters
]

results = [evaluate_logistic_model(C, solver, max_iter) for C, solver, max_iter in param_grid]

results_df = pd.DataFrame(results)

fig = px.scatter_3d(
    results_df,
    x="C",
    y="Max Iter",
    z="Solver",
    color="Test Accuracy",
    size="Test Accuracy",
    title="Hyperparameter Tuning Results for Logistic Regression",
    labels={"C": "C (Regularization)", "Max Iter": "Max Iterations", "Solver": "Solver"}
)
fig.show()

best_logistic_params = results_df.loc[results_df["Test Accuracy"].idxmax()]
print("Best Logic Regression Hyperparameters: ", best_logistic_params.to_dict())

"""## Match Outcome Prediction for Selected Picks

This section demonstrates predicting the winner of a match based on two pre-defined teams of heroes. The **Radiant** and **Dire** teams are specified by hero names, which are converted into their corresponding IDs using a helper function. The prediction model then analyzes the matchups, synergies, and win rates of the selected heroes to determine the likely winner and the confidence of the prediction.
"""

def predict_winner(radiant_team, dire_team, heroes_df, hero_matchups_df, hero_matchups_count_df, hero_synergies_df, hero_synergies_count_df, rfc):
    valid_radiant_team = [hero for hero in radiant_team if hero in hero_matchups_df.index]
    valid_dire_team = [hero for hero in dire_team if hero in hero_matchups_df.index]

    if len(valid_radiant_team) < 5 or len(valid_dire_team) < 5:
        raise ValueError("Each team must have 5 valid heroes.")

    test_match = {
        "match_id": 1,
        "radiant_team": valid_radiant_team,
        "dire_team": valid_dire_team,
        "radiant_wr": 0,
        "dire_wr": 0,
        "radiant_matchups_score": 0,
        "dire_matchups_score": 0,
        "radiant_synergies_score": 0,
        "dire_synergies_score": 0,
        "radiant_win": None
    }

    test_matches_df = pd.DataFrame([test_match])

    process_match_data(
        matches_df=test_matches_df,
        heroes_df=heroes_df,
        hero_matchups_df=hero_matchups_df,
        hero_matchups_count_df=hero_matchups_count_df,
        hero_synergies_df=hero_synergies_df,
        hero_synergies_count_df=hero_synergies_count_df
    )

    input_data = test_matches_df[['radiant_wr', 'dire_wr', 'radiant_matchups_score', 'dire_matchups_score', 'radiant_synergies_score', 'dire_synergies_score']]

    prediction = rfc.predict(input_data)
    probabilities = rfc.predict_proba(input_data)

    winner = "Radiant" if prediction[0] else "Dire"
    confidence = max(probabilities[0]) * 100

    return winner, confidence

def hero_names_to_ids(hero_names, heroes_df):
    hero_ids = []
    for name in hero_names:
        hero_id = heroes_df[heroes_df["name"] == name]["id"]
        if not hero_id.empty:
            hero_ids.append(hero_id.values[0])
        else:
            raise ValueError(f"Hero name '{name}' not found in the heroes_df.")
    return hero_ids

radiant_team_names = ["Anti-Mage", "Crystal Maiden", "Invoker", "Axe", "Lina"]
dire_team_names = ["Pudge", "Shadow Fiend", "Drow Ranger", "Juggernaut", "Vengeful Spirit"]

radiant_team = hero_names_to_ids(radiant_team_names, heroes_df)
dire_team = hero_names_to_ids(dire_team_names, heroes_df)

print("Radiant Team:")
for hero in radiant_team_names:
    print(hero)
print("\n-------------------------------\n")
print("Dire Team:")
for hero in dire_team_names:
    print(hero)

winner, confidence = predict_winner(
    radiant_team=radiant_team,
    dire_team=dire_team,
    heroes_df=heroes_df,
    hero_matchups_df=hero_matchups_df,
    hero_matchups_count_df=hero_matchups_count_df,
    hero_synergies_df=hero_synergies_df,
    hero_synergies_count_df=hero_synergies_count_df,
    rfc=rfc
)

print(f"\nPredicted winner: {winner} with confidence: {confidence:.2f}%")

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/

#!zip -r data.zip data/